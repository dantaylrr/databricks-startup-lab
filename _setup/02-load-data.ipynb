{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%md\n",
    "# Data initialization notebook. \n",
    "Do not run outside of the main notebook. This will automatically be called based on the reste_all widget value to setup the data required for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"true\", [\"true\", \"false\"], \"Reset all data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install importlib-metadata==6.8.0 cloudpickle==2.2.1 zipp==3.16.2\n",
    "%pip install azure-core azure-storage-file-datalake #for the display() in Azure only\n",
    "%pip install --ignore-installed Jinja2==3.1.2 markupsafe==2.1.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-schema-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(folder)\n",
    "  dbutils.fs.ls(folder+\"/orders\")\n",
    "  dbutils.fs.ls(folder+\"/users\")\n",
    "  dbutils.fs.ls(folder+\"/events\")\n",
    "  dbutils.fs.ls(folder+\"/ml_features\")\n",
    "  data_exists = True\n",
    "  print(\"Data already exists\")\n",
    "except Exception as e:\n",
    "  print(f\"Folder doesn't exist, generating the data...\")\n",
    "\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  # Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%md \n",
    "## Let's download the data from dbdemos resource repo\n",
    "If this fails, fallback on generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_downloaded = False\n",
    "if not data_exists:\n",
    "    try:\n",
    "        DBLab.download_file_from_git(folder+'/events', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/events\")\n",
    "        DBLab.download_file_from_git(folder+'/orders', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/orders\")\n",
    "        DBLab.download_file_from_git(folder+'/users', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/users\")\n",
    "        # DBLab.download_file_from_git(folder+'/ml_features', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/ml_features\")\n",
    "        data_downloaded = True\n",
    "    except Exception as e: \n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    \n",
    "else:\n",
    "    data_downloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if data_downloaded:\n",
    "    dbutils.notebook.exit(\"data downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "from collections import OrderedDict \n",
    "import uuid\n",
    "fake = Faker()\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake_firstname = F.udf(fake.first_name)\n",
    "fake_lastname = F.udf(fake.last_name)\n",
    "fake_email = F.udf(fake.ascii_company_email)\n",
    "\n",
    "def fake_date_between(months=0):\n",
    "  start = datetime.now() - timedelta(days=30*months)\n",
    "  return F.udf(lambda: fake.date_between_dates(date_start=start, date_end=start + timedelta(days=30)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "\n",
    "fake_date = F.udf(lambda:fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_date_old = F.udf(lambda:fake.date_between_dates(date_start=datetime(2012,1,1), date_end=datetime(2015,12,31)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_address = F.udf(fake.address)\n",
    "canal = OrderedDict([(\"WEBAPP\", 0.5),(\"MOBILE\", 0.1),(\"PHONE\", 0.3),(None, 0.01)])\n",
    "fake_canal = F.udf(lambda:fake.random_elements(elements=canal, length=1)[0])\n",
    "fake_id = F.udf(lambda: str(uuid.uuid4()) if random.uniform(0, 1) < 0.98 else None)\n",
    "countries = ['FR', 'USA', 'SPAIN']\n",
    "fake_country = F.udf(lambda: countries[random.randint(0,2)])\n",
    "\n",
    "def get_df(size, month):\n",
    "  df = spark.range(0, size).repartition(10)\n",
    "  df = df.withColumn(\"id\", fake_id())\n",
    "  df = df.withColumn(\"firstname\", fake_firstname())\n",
    "  df = df.withColumn(\"lastname\", fake_lastname())\n",
    "  df = df.withColumn(\"email\", fake_email())\n",
    "  df = df.withColumn(\"address\", fake_address())\n",
    "  df = df.withColumn(\"canal\", fake_canal())\n",
    "  df = df.withColumn(\"country\", fake_country())  \n",
    "  df = df.withColumn(\"creation_date\", fake_date_between(month)())\n",
    "  df = df.withColumn(\"last_activity_date\", fake_date())\n",
    "  df = df.withColumn(\"gender\", F.round(F.rand()+0.2))\n",
    "  return df.withColumn(\"age_group\", F.round(F.rand()*10))\n",
    "\n",
    "df_customers = get_df(133, 12*30).withColumn(\"creation_date\", fake_date_old())\n",
    "for i in range(1, 24):\n",
    "  df_customers = df_customers.union(get_df(2000+i*200, 24-i))\n",
    "\n",
    "df_customers = df_customers.cache()\n",
    "\n",
    "ids = df_customers.select(\"id\").collect()\n",
    "ids = [r[\"id\"] for r in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Number of order per customer to generate a nicely distributed dataset\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "mu, sigma = 3, 2 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, int(len(ids)))\n",
    "s = [i if i > 0 else 0 for i in s]\n",
    "\n",
    "# Most of our customers have ~3 orders\n",
    "import matplotlib.pyplot as plt\n",
    "count, bins, ignored = plt.hist(s, 30, density=False)\n",
    "plt.show()\n",
    "s = [int(i) for i in s]\n",
    "\n",
    "order_user_ids = list()\n",
    "action_user_ids = list()\n",
    "for i, id in enumerate(ids):\n",
    "  for j in range(1, s[i]):\n",
    "    order_user_ids.append(id)\n",
    "    #Let's make 5 more actions per order (5 click on the website to buy something)\n",
    "    for j in range(1, 5):\n",
    "      action_user_ids.append(id)\n",
    "\n",
    "print(f\"Generated {len(order_user_ids)} orders and  {len(action_user_ids)} actions for {len(ids)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id'])\n",
    "orders = orders.withColumn(\"id\", fake_id())\n",
    "orders = orders.withColumn(\"transaction_date\", fake_date())\n",
    "orders = orders.withColumn(\"item_count\", F.round(F.rand()*2)+1)\n",
    "orders = orders.withColumn(\"amount\", F.col(\"item_count\")*F.round(F.rand()*30+10))\n",
    "orders = orders.cache()\n",
    "orders.repartition(10).write.format(\"json\").mode(\"overwrite\").save(folder+\"/orders\")\n",
    "cleanup_folder(folder+\"/orders\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Website interaction\n",
    "import re\n",
    "\n",
    "platform = OrderedDict([(\"ios\", 0.5),(\"android\", 0.1),(\"other\", 0.3),(None, 0.01)])\n",
    "fake_platform = F.udf(lambda:fake.random_elements(elements=platform, length=1)[0])\n",
    "\n",
    "action_type = OrderedDict([(\"view\", 0.5),(\"log\", 0.1),(\"click\", 0.3),(None, 0.01)])\n",
    "fake_action = F.udf(lambda:fake.random_elements(elements=action_type, length=1)[0])\n",
    "fake_uri = F.udf(lambda:re.sub(r'https?:\\/\\/.*?\\/', \"https://databricks.com/\", fake.uri()))\n",
    "\n",
    "\n",
    "actions = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id']).repartition(20)\n",
    "actions = actions.withColumn(\"event_id\", fake_id())\n",
    "actions = actions.withColumn(\"platform\", fake_platform())\n",
    "actions = actions.withColumn(\"date\", fake_date())\n",
    "actions = actions.withColumn(\"action\", fake_action())\n",
    "actions = actions.withColumn(\"session_id\", fake_id())\n",
    "actions = actions.withColumn(\"url\", fake_uri())\n",
    "actions = actions.cache()\n",
    "actions.write.format(\"csv\").option(\"header\", True).mode(\"overwrite\").save(folder+\"/events\")\n",
    "cleanup_folder(folder+\"/events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's generate the Churn information. We'll fake it based on the existing data & let our ML model learn it\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "churn_proba_action = actions.groupBy('user_id').agg({'platform': 'first', '*': 'count'}).withColumnRenamed(\"count(1)\", \"action_count\")\n",
    "# Let's count how many order we have per customer.\n",
    "churn_proba = orders.groupBy('user_id').agg({'item_count': 'sum', '*': 'count'})\n",
    "churn_proba = churn_proba.join(churn_proba_action, ['user_id'])\n",
    "churn_proba = churn_proba.join(df_customers, churn_proba.user_id == df_customers.id)\n",
    "\n",
    "# Customer having > 5 orders are likely to churn\n",
    "churn_proba = (churn_proba.withColumn(\"churn_proba\", 5 +  F.when(((col(\"count(1)\") >=5) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 ((col(\"count(1)\") ==3) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"count(1)\") ==2) & (col(\"gender\") == 1) & (col(\"age_group\") <= 3)) |\n",
    "                                                                 ((col(\"sum(item_count)\") <=1) & (col(\"first(platform)\") == \"android\")) |\n",
    "                                                                 ((col(\"sum(item_count)\") >=10) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 (col(\"action_count\") >=4) |\n",
    "                                                                 (col(\"country\") == \"USA\") |\n",
    "                                                                 ((F.datediff(F.current_timestamp(), col(\"creation_date\")) >= 90)) |\n",
    "                                                                 ((col(\"age_group\") >= 7) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"age_group\") <= 2) & (col(\"gender\") == 1)), 80).otherwise(20)))\n",
    "\n",
    "churn_proba = churn_proba.withColumn(\"churn\", F.rand()*100 < col(\"churn_proba\"))\n",
    "churn_proba = churn_proba.drop(\"user_id\", \"churn_proba\", \"sum(item_count)\", \"count(1)\", \"first(platform)\", \"action_count\")\n",
    "churn_proba.repartition(100).write.format(\"json\").mode(\"overwrite\").save(folder+\"/users\")\n",
    "cleanup_folder(folder+\"/users\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
