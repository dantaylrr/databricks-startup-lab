{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%md \n",
    "\n",
    "# Technical Setup notebook. Hide this cell results\n",
    "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
    "\n",
    "Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"true\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"12.2\", \"Min required DBR version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "class DBLab():\n",
    "  @staticmethod\n",
    "  def setup_schema(catalog, db, reset_all_data, volume_name = None):\n",
    "    if reset_all_data:\n",
    "      print(f'clearing up volume named `{catalog}`.`{db}`.`{volume_name}`')\n",
    "      try:\n",
    "        spark.sql(f\"DROP VOLUME IF EXISTS `{catalog}`.`{db}`.`{volume_name}`\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS `{catalog}`.`{db}` CASCADE\")\n",
    "      except Exception as e:\n",
    "        print(f'catalog `{catalog}` or schema `{db}` do not exist.  Skipping data reset')\n",
    "\n",
    "    def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "      print(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "    assert catalog not in ['hive_metastore', 'spark_catalog', 'main'], \"This demo only supports Unity & a non-default catalog. Please change your catalog name.\"\n",
    "    #If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "    current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "    if current_catalog != catalog:\n",
    "      catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "      if catalog not in catalogs:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "        if catalog == 'dbdemos':\n",
    "          spark.sql(f\"ALTER CATALOG `{catalog}` OWNER TO `account users`\")\n",
    "    use_and_create_db(catalog, db)\n",
    "\n",
    "    if catalog == 'dbdemos':\n",
    "      try:\n",
    "        spark.sql(f\"GRANT CREATE, USAGE on DATABASE `{catalog}`.`{db}` TO `account users`\")\n",
    "        spark.sql(f\"ALTER SCHEMA `{catalog}`.`{db}` OWNER TO `account users`\")\n",
    "        for t in spark.sql(f'SHOW TABLES in {catalog}.{db}').collect():\n",
    "          try:\n",
    "            spark.sql(f'GRANT ALL PRIVILEGES ON TABLE {catalog}.{db}.{t[\"tableName\"]} TO `account users`')\n",
    "            spark.sql(f'ALTER TABLE {catalog}.{db}.{t[\"tableName\"]} OWNER TO `account users`')\n",
    "          except Exception as e:\n",
    "            if \"NOT_IMPLEMENTED.TRANSFER_MATERIALIZED_VIEW_OWNERSHIP\" not in str(e) and \"STREAMING_TABLE_OPERATION_NOT_ALLOWED.UNSUPPORTED_OPERATION\" not in str(e) :\n",
    "              print(f'WARN: Couldn t set table {catalog}.{db}.{t[\"tableName\"]} owner to account users, error: {e}')\n",
    "      except Exception as e:\n",
    "        print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "    print(f\"using catalog.database `{catalog}`.`{db}`\")\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{db}`\"\"\")     \n",
    "\n",
    "    if volume_name:\n",
    "      spark.sql(f'CREATE VOLUME IF NOT EXISTS {volume_name};')\n",
    "\n",
    "                     \n",
    "  # Return true if the folder is empty or does not exists\n",
    "  @staticmethod\n",
    "  def is_folder_empty(folder):\n",
    "    try:\n",
    "      return len(dbutils.fs.ls(folder)) == 0\n",
    "    except:\n",
    "      return True\n",
    "    \n",
    "  @staticmethod\n",
    "  def is_any_folder_empty(folders):\n",
    "    return any([DBLab.is_folder_empty(f) for f in folders])\n",
    "\n",
    "  @staticmethod\n",
    "  def download_file_from_git(dest, owner, repo, path):\n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192): \n",
    "            # If you have chunk encoded response uncomment if\n",
    "            # and set chunk_size parameter to None.\n",
    "            #if chunk: \n",
    "            f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "      try:\n",
    "        # Temporary fix to avoid hitting github limits - Swap github to our S3 bucket to download files\n",
    "        s3url = url.replace(\"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\", \"https://notebooks.databricks.com/demos/dbdemos-dataset/\")\n",
    "        download_file(s3url, dest)\n",
    "      except:\n",
    "        download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "      collections.deque(executor.map(download_to_dest, files))\n",
    "\n",
    "  @staticmethod\n",
    "  def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams_asynch(start_with = \"\", sleep_time=0):\n",
    "    import threading\n",
    "    def stop_streams():\n",
    "        DBLab.stop_all_streams(start_with=start_with, sleep_time=sleep_time)\n",
    "\n",
    "    thread = threading.Thread(target=stop_streams)\n",
    "    thread.start()\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "    import time\n",
    "    time.sleep(sleep_time)\n",
    "    streams = DBLab.get_active_streams(start_with)\n",
    "    if len(streams) > 0:\n",
    "      print(f\"Stopping {len(streams)} streams\")\n",
    "      for s in streams:\n",
    "          try:\n",
    "              s.stop()\n",
    "          except:\n",
    "              pass\n",
    "      print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_all_stream(start = \"\"):\n",
    "    import time\n",
    "    actives = DBLab.get_active_streams(start)\n",
    "    if len(actives) > 0:\n",
    "      print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "    while len(actives) > 0:\n",
    "      spark.streams.awaitAnyTermination()\n",
    "      time.sleep(1)\n",
    "      actives = DBLab.get_active_streams(start)\n",
    "    print(\"All streams completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's skip some warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
